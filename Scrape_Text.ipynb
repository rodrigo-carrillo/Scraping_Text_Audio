{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3be8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_pages_to_scrape = 10\n",
    "where_to_save_csv = \"/Users/rodrigocarrillo/Documents/Natural Language Processing Projects/Rotafono Scrape/01_Data_Text/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d64ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_rotafono_selenium(max_pages=None):\n",
    "    \"\"\"\n",
    "    Scrape rotafono news using Selenium for JavaScript-rendered content\n",
    "    \"\"\"\n",
    "    \n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        page_num = 1\n",
    "        \n",
    "        while True:\n",
    "            # Build URL with correct parameter structure: ?page=X&departamento=lima\n",
    "            url = f\"https://rotafono.pe/casos/?page={page_num}&departamento=lima\"\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Scraping page {page_num}: {url}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for content to load\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a[href*='/casos/']\"))\n",
    "                )\n",
    "            except:\n",
    "                print(\"Timeout waiting for content to load\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Get all article links\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find all links that point to specific cases (structure: /casos/<district>-...)\n",
    "            # This filters out pagination and other generic links\n",
    "            # Note: cases can be from any district, not just lima (e.g., /casos/carabayllo-..., /casos/chorrillos-...)\n",
    "            case_links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href', '')\n",
    "                # Look for links with pattern: /casos/<case-name>-<id>/\n",
    "                # Match /casos/ followed by a district name and ends with /\n",
    "                # Exclude generic /casos/ or /casos/ with only one part\n",
    "                if href.startswith('/casos/') and href.endswith('/') and href != '/casos/':\n",
    "                    # Additional check: must have at least one hyphen to be a case link\n",
    "                    path_part = href[7:-1]  # Remove /casos/ prefix and trailing /\n",
    "                    if '-' in path_part:\n",
    "                        case_links.append(link)\n",
    "            \n",
    "            if not case_links:\n",
    "                print(\"No article links found on this page\")\n",
    "                break\n",
    "            \n",
    "            print(f\"Found {len(case_links)} links on page {page_num}\")\n",
    "            \n",
    "            # Deduplicate articles on this page by URL first\n",
    "            # Store {url: title} to get the best (non-empty) title for each URL\n",
    "            url_to_article = {}\n",
    "            \n",
    "            for link in case_links:\n",
    "                title = link.get_text(strip=True)\n",
    "                href = link.get('href', '')\n",
    "                \n",
    "                if not href:\n",
    "                    continue\n",
    "                \n",
    "                # Convert relative URLs to absolute\n",
    "                if href.startswith('/'):\n",
    "                    href = 'https://rotafono.pe' + href\n",
    "                elif not href.startswith('http'):\n",
    "                    href = 'https://rotafono.pe/' + href\n",
    "                \n",
    "                # Keep the first non-empty title for each URL\n",
    "                if href not in url_to_article:\n",
    "                    url_to_article[href] = title\n",
    "                elif len(title) > len(url_to_article[href]):\n",
    "                    # Update if we find a longer (likely more complete) title\n",
    "                    url_to_article[href] = title\n",
    "            \n",
    "            # Create articles list from deduplicated URLs\n",
    "            page_articles = []\n",
    "            for url, title in url_to_article.items():\n",
    "                # Skip if title is empty or too short\n",
    "                if not title or len(title) < 5:\n",
    "                    continue\n",
    "                \n",
    "                page_articles.append({\n",
    "                    'title': title,\n",
    "                    'url': url,\n",
    "                    'page': page_num\n",
    "                })\n",
    "            \n",
    "            articles.extend(page_articles)\n",
    "            print(f\"Added {len(page_articles)} unique articles to results\")\n",
    "            \n",
    "            for i, article in enumerate(page_articles, 1):\n",
    "                print(f\"  {i}. {article['title'][:70]}...\")\n",
    "            \n",
    "            # Check for next page button or link\n",
    "            next_button = None\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href', '')\n",
    "                link_text = link.get_text(strip=True)\n",
    "                # Check if link points to next page using correct parameter structure\n",
    "                if f'page={page_num + 1}' in href and 'departamento=lima' in href:\n",
    "                    next_button = link\n",
    "                    break\n",
    "            \n",
    "            # Stop if max_pages reached\n",
    "            if max_pages and page_num >= max_pages:\n",
    "                print(f\"\\nReached maximum pages limit ({max_pages})\")\n",
    "                break\n",
    "            \n",
    "            # Stop if no next page\n",
    "            if not next_button:\n",
    "                print(f\"\\nNo more pages found\")\n",
    "                break\n",
    "            \n",
    "            page_num += 1\n",
    "            time.sleep(5)  # Be respectful to the server\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    df = pd.DataFrame(articles)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start scraping\n",
    "print(\"Starting Selenium-based scraper...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "articles_df = scrape_rotafono_selenium(max_pages = how_many_pages_to_scrape)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SCRAPING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total articles scraped: {len(articles_df)}\")\n",
    "print(f\"\\nFirst 15 articles:\")\n",
    "print(articles_df.head(15).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0082510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_content(article_url):\n",
    "    \"\"\"\n",
    "    Scrape the full content of a specific article\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(article_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract article title\n",
    "        title_elem = soup.find('h1', class_='title')\n",
    "        title_text = title_elem.get_text(strip=True) if title_elem else \"N/A\"\n",
    "        \n",
    "        # Extract article category\n",
    "        category_elem = soup.find('h2', class_='category')\n",
    "        category = category_elem.get_text(strip=True) if category_elem else \"\"\n",
    "        \n",
    "        # Extract publish date\n",
    "        date_elem = soup.find('time', class_='date-published')\n",
    "        publish_date = date_elem.get_text(strip=True) if date_elem else \"\"\n",
    "        \n",
    "        # Extract full article content from the main case container\n",
    "        content_text = \"\"\n",
    "        \n",
    "        # Get the main article container\n",
    "        main_case = soup.find('div', class_='container-main-case')\n",
    "        \n",
    "        if main_case:\n",
    "            # Extract all text from the card div (which contains the main content)\n",
    "            card = main_case.find('div', class_='card')\n",
    "            \n",
    "            if card:\n",
    "                # Get all paragraphs and other text elements\n",
    "                paragraphs = card.find_all(['p', 'h2', 'h3', 'h4'])\n",
    "                \n",
    "                content_parts = []\n",
    "                \n",
    "                for elem in paragraphs:\n",
    "                    # Skip navigation and social elements\n",
    "                    if 'question' in elem.get('class', []) or 'answer' in elem.get('class', []):\n",
    "                        # Handle Q&A format\n",
    "                        if 'question' in elem.get('class', []):\n",
    "                            content_parts.append(f\"\\n{elem.get_text(strip=True)}\\n\")\n",
    "                        else:\n",
    "                            content_parts.append(elem.get_text(strip=True))\n",
    "                    elif elem.name in ['h2', 'h3', 'h4']:\n",
    "                        # Add headings with formatting\n",
    "                        content_parts.append(f\"\\n\\n{elem.get_text(strip=True)}\\n\")\n",
    "                    elif 'figcaption' not in str(elem.get('class', [])):\n",
    "                        # Regular paragraphs\n",
    "                        text = elem.get_text(strip=True)\n",
    "                        if text and len(text) > 3:\n",
    "                            content_parts.append(text)\n",
    "                \n",
    "                content_text = \"\\n\\n\".join(content_parts)\n",
    "            \n",
    "            # If no content from card, get from main case container directly\n",
    "            if not content_text:\n",
    "                content_text = main_case.get_text(strip=True)\n",
    "        \n",
    "        # Clean up excessive whitespace\n",
    "        if content_text:\n",
    "            content_text = '\\n\\n'.join([line.strip() for line in content_text.split('\\n') if line.strip()])\n",
    "        else:\n",
    "            content_text = \"Content not found\"\n",
    "        \n",
    "        return {\n",
    "            'title': title_text,\n",
    "            'url': article_url,\n",
    "            'category': category,\n",
    "            'publish_date': publish_date,\n",
    "            'content': content_text,\n",
    "            'full_content': content_text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'title': \"Error\",\n",
    "            'url': article_url,\n",
    "            'category': \"\",\n",
    "            'publish_date': \"\",\n",
    "            'content': str(e),\n",
    "            'full_content': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d32af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape content for all URLs in articles_df\n",
    "def scrape_and_extract(row):\n",
    "    article_content_raw = scrape_article_content(row['url'])\n",
    "    return pd.Series({\n",
    "        'category': article_content_raw['category'],\n",
    "        'publish_date': article_content_raw['publish_date'],\n",
    "        'article_content': article_content_raw['content']\n",
    "    })\n",
    "\n",
    "articles_df[['category', 'publish_date', 'article_content']] = articles_df.apply(scrape_and_extract, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf866b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "today_str = datetime.date.today().strftime(\"%Y%m%d\")\n",
    "articles_df.to_pickle(where_to_save_csv + f\"rotafono_articles_scraped_{today_str}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doctoralia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
